%% Resources

% For training and stuff: http://neuralnetworksanddeeplearning.com/chap3.html


% Could summarize this under a bigger chapter:
% DATA ANALYSIS STRATEGIES 

% Intro: Describe dimensionality reduction.

% Machine Learning
% Statistical data analysis (binned likelihood function)

% %---------------------------------------------------------------------
\chapter{Machine Learning in High Energy Physics}
\label{chap:ml}

In recent years, machine learning (ML) applications became ubiquitous and - often unnoticed - drive much of humans' decision making. 
Also HEP entered the data-driven era and more and more traditional analysis techniques have been replaced by modern ML tools.
ML tools can analyse high-dimensional data simultaneously which in many cases makes them superior to traditional analysis strategies that often rely on boolean selections on single observables. 

A formal definition of ML was provided in the book ``Machine Learning''~\ccite{mitchell1997machine}:
``A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.''
This definition can be interpreted from the perspective of supervised learning techniques, that are particularly common in HEP physics analyses. 
Supervised learning relies on labelled sample data which in HEP is provided by MC simulated samples that have ground-truth labels and thus provide the inputs - or ``experience'' - for a computer program to ``learn''.
The ``tasks T'' in HEP often correspond to classification tasks, the ``performance P'' of which can be measured with a metric that compares the true label with the label predicted by the model. 
The iterative updating of the model that is being learned by the ``computer program'', so that the metric is optimized, can be thought of as the ``learning'' process, also known as ``training''. 

This chapter first provides an overview of the various applications of ML in HEP that exist to date and then discusses the core concepts behind it. Thereafter, the details of ML that are relevant for the work presented in this thesis are discussed, in particular the specifics behind the concept of neural networks, and the details of the learning procedure.

\Cref{chap:hww} of this thesis discusses, how an ML model is trained to classify signal-like and background-like $pp$ collision events. 

\TDinote{}{Add references to sections}
\TDinote{}{Maybe introduce neural networks already here!}
\TDinote{}{Not sure if I need the blah blah above}

% Description of cut-based analysis
% Traditional data analysis techniques in HEP use a sequence of boolean deci- sions followed by statistical analysis on the selected data. Typically, both the individual decisions and the subsequent statistical analysis are based on the distribution of a single ob- served quantity motivated by physics considerations, which is not easily extended to higher dimensions.
% Within HEP this approach is often referred to as multivariate analysis (MVA); however, outside of physics these techniques would be considered examples of machine learning.

\section{Applications of Machine Learning in HEP}
- ML is a fast moving field and more and more applications are being tested and adopted in HEP. The following provides only a very small selection of potential applications in HEP and aims at highlighting the different areas in HEP where ML models can be leveraged. 

- Different applications of ML in ATLAS and throughout HEP community in general: hit reconstruction \ccite{PERF-2012-05} or track finding, object identification, classification of entire events for example in the search for SUSY particles, New calibration techniques (general jet calibration) or tagging techniques (W Z boson tagging?). 
% check here for references: https://arxiv.org/pdf/2103.12226.pdf

\TDinote{}{Think whether this is really needed! Leave out for now! Maybe change introduction}

\section{Core Concepts and Terminology in Machine Learning}
The basic task of a machine learning algorithm is to find a function $f: X \rightarrow Y$, that maps some higher-dimensional data $X$ onto some target label $Y$, while optimizing a metric of choosing.
This metric is known as \emph{loss function} (also \emph{cost function}), $L(y, f(x))$.
The function that optimizes the loss function is found in a process referred to as \emph{training}, in which \TDnote{a learning algorithm is applied to a set of sample data}{Not really sure if that's a good way to say it}, known as \emph{training data}.
The space of functions to choose from is constrained by a \emph{model} that can come in various forms, such as boosted decision trees (BDTs), support vector machines (SVMs), or neural networks (NNs) (see \cref{sec:intro-neural-nets}). The latter, in particular deep neural networks (DNNs), are by now the most powerful machine learning models and ubiquitous in HEP and many other technology-related fields. \TDnote{[XX]}{Add reference to footnote?} \footnote{The advent of deep neural networks and machine learning in general was facilitated by the growing amount of both computing resources and data, as well as software advancements, in the beginning of the 21$^{\text{th}}$ century.}
%Details on DNNs are given below, as they are used in the work presented in this thesis.
% % From [1] see above:
% - Search for function f : X -> Y, that maps the higher-dimensional observed data X on some target label Y of lower dimension, while optimizing a metric of choosing. 
% - The metric is known as loss function and labelled as L(y, f(x)). 

% - In a process known as \emph{training}, a learning algorithm is applied to find the function that optimizes the loss function.

% - The space of functions to choose from is constrained by a model that can come in various forms such as BDTs, support vector machines, or neural networks (see the following chapter).

A major goal in machine learning is \emph{generalization}, that is the ability of a trained model to perform well on previously unseen data.
The opposite is the case if model parameters are chosen based on signatures unique to the training data. This is known as \emph{overtraining} (or \emph{overfitting}). To prevent a model to be overfitted, various so-called \emph{regularization} techniques can be used during the training.
Regularization methods are typically improved during the training procedure in which other parameters are also optimized, which is explained further below.
%based on how well a trained model generalizes, which can be tested with data not used in the training.


\section{Introduction to Neural Networks}
\label{sec:intro-neural-nets}
Neural networks consist of interconnected layers of artificial neurons\footnote{ NNs are therefore also called artificial neural networks (ANNs), which is more descriptive but usually used synonymously with the simple form ``neural networks''.}. An illustration of a fully-connected neural network, where each layer receives inputs only from the previous layer, is shown in \cref{fig:neural-net}. These types of NNs are known as \emph{feedforward} NNs.
The first layer is called the \emph{input layer} and corresponds to the input vector, $\mathbf{x}$. The last layer is called the \emph{output layer}, $\mathbf{y}$, and the layers in between are known as \emph{hidden layers}. Each neuron performs a transformation of the data from the respective previous layer based on an \emph{activation function}, labelled as $h^{(l)}(\cdot)$ for the $l^\text{th}$ layer.
The activation function of the first layer can thus be considered as the identity function, $h^{(0)}(\mathbf{x}) \equiv \mathbf{x}$.
The artificial neurons are connected via \emph{links} that have weights associated.
A weight labelled with $w^{(l)}_{jk}$ connects the $k^\text{th}$ neuron in the $l^\text{th}$ layer to the $j^\text{th}$ neuron in $(l+1)^\text{th}$ layer.
In addition, each neuron except the ones in the input layer has a so-called \emph{bias} weight, labelled as $w^{(l)}_{j0}$ for the $j^\text{th}$ neuron in the $l^\text{th}$ layer.
%A neuron is ``activated'' by the data coming from the respective previous layer. 
The activation of the $j^\text{th}$ neuron in the $l^{\text{th}}$ layer can then be described by a recursive relation,
\begin{equation}
    h^{(l)}_j =  h^{(l)} \left( \sum_{k}   w^{(l)}_{jk} h^{(l-1)}_{k}  + w^{(l)}_{j0} \right),
\end{equation}
where the sum is over all neurons $k$ in the $(l-1)^\text{th}$ layer.
% While the number of neurons in the input and output layer are typically chosen based on the problem at hand, the specifics for the hidden layers are usually optimized empirically.  
This equation can be written in matrix form to describe the transformation of the entire layer,
\begin{equation}
    \label{eq:recursive-neuron-activation}
    \mathbf{h}^{(l)} =  h^{(l)} \left( W^{(l)} \mathbf{h}^{(l-1)}  + \mathbf{w}^{(l)}_{0} \right),
\end{equation}
by defining the weight matrix $W^{(l)} = \left(w^{(l)}_{jk} \right)$, and the vectors corresponding to the activations and bias terms as $\mathbf{h}^{(l)}$ and  $\mathbf{w}^{(l)}_0$, respectively.
%\Cref{eq:recursive-neuron-activation} is thus a lin
% Taking a simple network with $N$ inputs, one hidden layer with $H$ artificial neurons, and a single output neuron, the full neural network function can be written as,
% \begin{equation}
%     y(\mathbf{x}, \mathbf{W}) = \sigma \left( \sum_{h=1}^H w^{(2)}_{kh}h^{(1)}\left(  \sum_{n=1}^{N} w^{(1)}_{hn}x_n + w^{(1)}_{h0}    \right) + w^{(2)}_{0} \right),
% \end{equation}
The full function of a simple network with one hidden layer and a single output neuron can then be written as,
\begin{equation}
    y(\mathbf{x}, \mathbf{W}) = h^{(2)} \left( W^{(2)} h^{(1)} \left(  W^{(1)} \mathbf{x} + \mathbf{w}^{(1)}_{0}    \right) + w^{(2)}_{0} \right),
\end{equation}
which simply corresponds to a nonlinear function, controlled by a set of adjustable parameters of weights and bias weights, that are together labelled as $\mathbf{W}$. Here, the weight matrix $W^{(2)}$ is a vector and the activation function $h^{(2)}(\cdot)$ is the activation of the output neuron.

% In order to understand this transformation, a single artificial neuron is depicted in \cref{fig:single-neuron}. 
The type of activation function can have different forms; typical choices are a logistic sigmoid,
\begin{equation}
    \label{eq:logistic-sigmoid}
    h(x)= \sigma(x) = {\frac {1}{1+e^{-x}}},
\end{equation}
or a so-called Rectified Linear Unit (ReLU) function, defined as
\begin{equation}
    {h(x)= R(x) = \max(0,x)}.    
\end{equation}
\Minote{}{Maybe remove the specifics and move them to the next chapter.}
The same activation function is usually used across all hidden layers.
The output layer typically has a dedicated activation function, tailored to the problem at hand.

The choice of activation functions as well as the number of layers and neurons is known as the \emph{network architecture} in a feedforward NN. 
%The weights $\mathbf{W}$ of the NN are the free parameters that are to be optimized during the training process.

% Quote from: http://neuralnetworksanddeeplearning.com/chap1.html
%- Up to now, we've been discussing neural networks where the output from one layer is used as input to the next layer. Such networks are called feedforward neural networks.



\begin{figure}[t]
    \newImageResizeCustom{0.9}{figures/data-analysis/neural-network-architecture.png}
    \caption{Example illustration of a fully-connected neural network. The three dots indicate that neural networks can be defined with an arbitrary number of neurons and hidden layers.}
    \label{fig:neural-net}
\end{figure}


% \begin{figure}[t]
%     \newImageResizeCustom{0.7}{figures/data-analysis/single-neuron-illustration.png}
%     \caption{Illustration of a single artificial neuron, including the inputs, $\mathbf{x}$ and $\mathbf{w}$, the bias, $b$, and the activation function $a(\cdot)$ as well as output weight.}
%     \label{fig:single-neuron}
% \end{figure}


\section{Neural Network Training}
% Read also here: http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent

% The work presented in this thesis uses supervised learning techniques to train a binary classifier that distinguishes between signal-like and background-like events. 
% Therefore, only a single output node is required, that uses a logistic sigmoid as activation function such that $y = \sigma(x)$, where $\sigma(x)$ is defined as in \cref{eq:logistic-sigmoid}.
% The network is trained with MC simulated $pp$ collision events that have ground-truth labels called \emph{target values} in the following. The target value, $t_n$, for a given event $n$ has a value of $t_n = 1$, if the event is a simulated signal event, and $t_n = 0$ otherwise.
% \Minote{}{The above can potentially go into the HWW chapter, as it is specific! Would need to define target values}

The training process is an iterative numerical procedure, during which the free parameters of the network, $\mathbf{W}$, are updated so that the loss function, $L(\mathbf{W})$, is minimized.

To determine the weight updates, gradient-based optimization algorithms are commonly used, that update a given weight based on the gradient of the loss function with respect to that weight, 
\begin{equation}
    \label{eq:gradient-descent}
    w^{(\tau+1)} = w^\tau - \eta \grad_{w^{(\tau)}} L(\mathbf{W}),
\end{equation}
where $\eta$ is known as the \emph{learning rate}. 
The initial weights, $w^{(0)}$, are chosen randomly.
The weights are usually updated after a \emph{batch} of training samples, which is known as \emph{mini-batch gradient descent}. 
After each batch of data, the loss is computed and the weights are adjusted according to \cref{eq:gradient-descent}, which requires the computation of the gradient of the loss function. 
%The most widely used optimization algorithm is the \emph{gradient descent} or \emph{stochastic gradient descent} algorithm. 
This is a non-trivial task as it cannot be solved analytically. 
A method known as \emph{back propagation}~\ccite{Rumelhart_1986} is therefore used, that propagates the gradients to each weight.  

In supervised learning, the loss function typically quantifies the difference between the true values of the labelled sample data -- known as \emph{target values}, $t_n$ -- and the values predicted by the current NN model. A typical choice is the sum-of-squares loss function,
\begin{equation}
   L(\mathbf{W}) = \sum _{n=1}^{N}\left( y(\mathbf{x}_n, \mathbf{W})-t_n \right)^{2},
\end{equation}
where $N$ is the size of the batch, denoted as \emph{batchsize}. 
Another commonly used loss function especially for classification tasks is the so-called \emph{cross-entropy loss},
\begin{equation}
    L(\mathbf{W}) = \sum _{n=1}^{N}\left( t_n \ln y_n + ( 1 - t_n) \ln (1 - y_n) \right).
\end{equation}


\section{Hyperparameter optimization} 
In practice, when training a neural network, significant effort needs to be put into finding a suitable parameter set (known as \emph{hyperparameters}) so that the performance metric is optimized and the network converges to a minimum both rapidly and reliably. 
Several useful techniques exist that simplify hyperparameter optimization, as well as other training methods are explained below.

\paragraph{Learning rate} \mbox{}\\
The learning rate is one of the most crucial hyperparameters as it determines the stepsize of the weight updates. Choosing a learning rate that is too large, can prevent convergence and cause the loss function to fluctuate around the minimum. Too low a learning rate can lead to extremely slow or no convergence. 
One method to mitigate this problem is using a \emph{learning rate schedule}, that adjusts the learning rate based on the progress during the training. As such, a larger learning rate can be used in the beginning of the training, that gets smaller over time.
Typically, the convergence is tested when different learning rates and learning rate schedules are used.

% @see: https://ruder.io/optimizing-gradient-descent/
% avoiding getting trapped in their numerous suboptimal local minima
% A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.

\paragraph{Optimizer}\mbox{}\\
There are many variations of the gradient descent algorithm, known as \emph{gradient descent optimization algorithms}, that can improve the convergence of the training \ccite{ruder2017overview}. The \emph{AdaGrad}~\ccite{adagrad-duchi} algorithm is one such example. It adapts the learning rate based on the size of the gradients at each parameter, which, in layman's words, leads to rare features in the training data being given more weight than frequently occurring features. This helps to make the training converge faster and more reliably.


\paragraph{Dataset split and cross-validation}\mbox{}\\
The trained model must generalize well, which can be tested by splitting the dataset into multiple parts. 
This is often included in a proper performance metric based on which the final model selection is performed. 

- k-fold cross validation to prevent any bias but still exploit significant portion of available data set for training \cref{fig:k-fold-method}.
- CV per definition removes any potential bias that may occur when data is evaluated with a model that was trained with same data.

Train set: train data
Validation set: used during training to ``monitor'' the progress (e.g. for learning rate schedule) of each fold and choose set of hyperparameters (type of input, network architecture, learning rate, regularization technique, optimizers)
Test set: Used for final evaluation of training and test generalizability

- The set of parameters that define the network architecture together with the parameters used in the training are together referred to as hyperparameters.

- grid search

\paragraph{Regularisation}\mbox{}\\

- Regularisation techniques: dropout
- Early stopping

%\section{Multiclass Classification}

\begin{figure}[t]
    \newImageResizeCustom{0.7}{figures/data-analysis/k-fold-illustration.png}
    \caption{Schematic showing the split of the training data into a training (train), validation (val), and test set in a 5-fold cross-validation with interleaved validation and test set.}
    \label{fig:k-fold-method}
\end{figure}
