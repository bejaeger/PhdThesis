%% Resources

% For training and stuff: http://neuralnetworksanddeeplearning.com/chap3.html


% Could summarize this under a bigger chapter:
% DATA ANALYSIS STRATEGIES 

% Intro: Describe dimensionality reduction.

% Machine Learning
% Statistical data analysis (binned likelihood function)

% %---------------------------------------------------------------------
\chapter{Machine Learning in High Energy Physics}
\label{chap:ml}

In recent years, machine learning (ML) applications became ubiquitous and - often unnoticed - drive much of humans' decision making. 
Also HEP entered the data-driven era and more and more traditional analysis techniques have been replaced by modern ML tools.
% - Add examples in one/two sentences here?
% - mention classification task: 
% FROM PHD: "and an event-level classifier using the 4-momenta of a particle collision to determine which process gave rise to the collision."
% - mention supervised learning
% - mention neural networks
ML tools can analyse high-dimensional data simultaneously which in many cases makes them superior to traditional analysis strategies that often rely on boolean selections on single observables. 

A formal definition of ML was provided in the book ``Machine Learning''~\ccite{mitchell1997machine}:
``A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.''
This definition can be interpreted from the perspective of supervised learning techniques, that are particularly common in HEP physics analyses. 
Supervised learning relies on labelled sample data which in HEP is provided by MC simulated samples that have ground-truth labels and thus provide the inputs - or ``experience'' - for a computer program to ``learn''.
The ``tasks T'' in HEP often correspond to classification tasks, the ``performance P'' of which can be measured with a metric that compares the true label with the label predicted by the model. 
The iterative updating of the model that is being learned by the ``computer program'', so that the metric is optimized, can be thought of as the ``learning'' process, also known as ``training''. 

This chapter first provides an overview of the various applications of ML in HEP that exist to date and then discusses the core concepts behind it. Thereafter, the details of ML that are relevant for the work presented in this thesis are discussed, in particular the specifics behind the concept of neural networks, and the details of the learning procedure.

\Cref{chap:hww} of this thesis discusses, how an ML model is trained to classify signal-like and background-like $pp$ collision events. 

\TDinote{}{Add references to sections}
\TDinote{}{Maybe introduce neural networks already here!}
\TDinote{}{Not sure if I need the blah blah above}

% Description of cut-based analysis
% Traditional data analysis techniques in HEP use a sequence of boolean deci- sions followed by statistical analysis on the selected data. Typically, both the individual decisions and the subsequent statistical analysis are based on the distribution of a single ob- served quantity motivated by physics considerations, which is not easily extended to higher dimensions.
% Within HEP this approach is often referred to as multivariate analysis (MVA); however, outside of physics these techniques would be considered examples of machine learning.

\section{Applications of Machine Learning in HEP}
- ML is a fast moving field and more and more applications are being tested and adopted in HEP. The following provides only a very small selection of potential applications in HEP and aims at highlighting the different areas in HEP where ML models can be leveraged. 

\Minote{}{Put the above in a footnote in the introduction!}

- Different applications of ML in ATLAS and throughout HEP community in general: hit reconstruction \ccite{PERF-2012-05} or track finding, object identification, classification of entire events for example in the search for SUSY particles, New calibration techniques (general jet calibration) or tagging techniques (W Z boson tagging?). 
% check here for references: https://arxiv.org/pdf/2103.12226.pdf

\TDinote{}{Think whether this is really needed! Leave out for now! Maybe change introduction}

\section{Core Concepts and Terminology in Machine Learning}
The basic task of a machine learning algorithm is to find a function $f: X \rightarrow Y$, that maps some higher-dimensional data $X$ onto some target label $Y$, while optimizing a metric of choosing.
This metric is known as \emph{loss function} (also \emph{cost function}), $L(y, f(x))$.
The function that optimizes the loss function is found in a process referred to as \emph{training}, in which \TDnote{a learning algorithm is applied to a set of sample data}{Not really sure if that's a good way to say it}, known as \emph{training data}.
The space of functions to choose from is constrained by a \emph{model} with adjustable parameters that are optimized during the training. Examples of such models include: boosted decision trees (BDTs), support vector machines (SVMs), or neural networks (NNs) (see \cref{sec:intro-neural-nets}). The latter, in particular deep neural networks (DNNs), are the most widely used machine learning models today and ubiquitous in HEP and many other technology-related fields due to their great flexibility and ability to approximate more complex, non-linear functions. \TDnote{[XX]}{Add reference to footnote?} \footnote{The advent of deep neural networks and machine learning in general was facilitated by the growing amount of both computing resources and data, as well as software advancements, in the beginning of the 21$^{\text{th}}$ century.}
%Details on DNNs are given below, as they are used in the work presented in this thesis.
% % From [1] see above:
% - Search for function f : X -> Y, that maps the higher-dimensional observed data X on some target label Y of lower dimension, while optimizing a metric of choosing. 
% - The metric is known as loss function and labelled as L(y, f(x)). 

% - In a process known as \emph{training}, a learning algorithm is applied to find the function that optimizes the loss function.

% - The space of functions to choose from is constrained by a model that can come in various forms such as BDTs, support vector machines, or neural networks (see the following chapter).

A major goal in machine learning is \emph{generalization}, that is the ability of a trained model to perform well on previously unseen data.
The opposite is the case if model parameters are chosen based on signatures unique to the training data. This is known as \emph{overtraining} (or \emph{overfitting}). To prevent a model to be overfitted, various so-called \emph{regularization} techniques can be used during the training.
Regularization methods are typically improved during the training procedure in which other parameters are also optimized, which is explained further below.
%based on how well a trained model generalizes, which can be tested with data not used in the training.


\section{Introduction to Neural Networks}
\label{sec:intro-neural-nets}
Neural networks can be visualized as interconnected layers of artificial neurons\footnote{ NNs are therefore also called artificial neural networks (ANNs), which is more descriptive but usually used synonymously with the simple form ``neural networks''.}. An illustration of a fully-connected neural network, where each layer receives inputs only from the previous layer, is shown in \cref{fig:neural-net}. These types of NNs are known as \emph{feedforward} NNs.
The first layer is called the \emph{input layer} and corresponds to the input vector, $\mathbf{x}$. The last layer is called the \emph{output layer}, $\mathbf{y}$, and the layers in between are known as \emph{hidden layers}. Each neuron performs a transformation of the data from the respective previous layer based on an \emph{activation function}, labelled as $h^{(l)}(\cdot)$ for the $l^\text{th}$ layer.
Hence, the activation function of the input layer can be considered as the identity function, $h^{(0)}(\mathbf{x}) \equiv \mathbf{x}$.
The artificial neurons are connected via \emph{links} that have weights associated.
In the following notation, a weight labelled with $w^{(l)}_{jk}$ connects the $k^\text{th}$ neuron in the $l^\text{th}$ layer to the $j^\text{th}$ neuron in $(l+1)^\text{th}$ layer.
In addition, each neuron except the ones in the input layer has a so-called \emph{bias} weight, labelled as $w^{(l)}_{j0}$ for the $j^\text{th}$ neuron in the $l^\text{th}$ layer.
%A neuron is ``activated'' by the data coming from the respective previous layer. 
The activation of the $j^\text{th}$ neuron in the $l^{\text{th}}$ layer can then be described by a recursive relation,
\begin{equation}
    h^{(l)}_j =  h^{(l)} \left( \sum_{k}   w^{(l)}_{jk} h^{(l-1)}_{k}  + w^{(l)}_{j0} \right),
\end{equation}
where the sum is over all neurons $k$ in the $(l-1)^\text{th}$ layer.
% While the number of neurons in the input and output layer are typically chosen based on the problem at hand, the specifics for the hidden layers are usually optimized empirically.  
This equation can be written in matrix form to describe the transformation of the entire layer,
\begin{equation}
    \label{eq:recursive-neuron-activation}
    \mathbf{h}^{(l)} =  h^{(l)} \left( W^{(l)} \mathbf{h}^{(l-1)}  + \mathbf{w}^{(l)}_{0} \right),
\end{equation}
by defining the weight matrix $W^{(l)} = \left(w^{(l)}_{jk} \right)$, and the vectors corresponding to the activations and bias terms as $\mathbf{h}^{(l)}$ and  $\mathbf{w}^{(l)}_0$, respectively.
%\Cref{eq:recursive-neuron-activation} is thus a lin
% Taking a simple network with $N$ inputs, one hidden layer with $H$ artificial neurons, and a single output neuron, the full neural network function can be written as,
% \begin{equation}
%     y(\mathbf{x}, \mathbf{W}) = \sigma \left( \sum_{h=1}^H w^{(2)}_{kh}h^{(1)}\left(  \sum_{n=1}^{N} w^{(1)}_{hn}x_n + w^{(1)}_{h0}    \right) + w^{(2)}_{0} \right),
% \end{equation}
The full function of a simple network with one hidden layer and a single output neuron can then be written as,
\begin{equation}
    y(\mathbf{x}, \mathbf{W}) = h^{(2)} \left( W^{(2)} h^{(1)} \left(  W^{(1)} \mathbf{x} + \mathbf{w}^{(1)}_{0}    \right) + w^{(2)}_{0} \right),
\end{equation}
which simply corresponds to a nonlinear function, controlled by a set of adjustable parameters of weights and bias weights (collectively referred to as weights and labelled as $\mathbf{W}$ in the following). Here, the weight matrix $W^{(2)}$ is a vector and the activation function $h^{(2)}(\cdot)$ is the activation of the output neuron.

\TDinote{}{Maybe write about matrix from right away, avoid index war, and talk about transformations of LAYERS rather than single neurons}

% In order to understand this transformation, a single artificial neuron is depicted in \cref{fig:single-neuron}. 
There are different forms of activation functions that can be used.
Feedforward NNs generally use the same function in all hidden layers.
A common choice is the so-called \emph{Rectified Linear Unit} (ReLU) function, defined as
\begin{equation}
    {h(x)= R(x) = \max(0,x)}.    
\end{equation}
A dedicated activation function is typically used for the output node, dependent on the problem.
A natural choice for binary classification is a logistic sigmoid,
\begin{equation}
    \label{eq:logistic-sigmoid}
    h(x)= \sigma(x) = {\frac {1}{1+e^{-x}}},
\end{equation}
which restricts the output to values between zero and one and can thus be interpreted probabilistically.
All activation functions must be continuously differentiable, in order for the training algorithm to succeed.

\TDinote{}{Add picture of sigmoid.}

% The choice of activation functions as well as the number of layers and neurons is known as the \emph{network architecture} in a feedforward NN. 
%The weights $\mathbf{W}$ of the NN are the free parameters that are to be optimized during the training process.

% Quote from: http://neuralnetworksanddeeplearning.com/chap1.html
%- Up to now, we've been discussing neural networks where the output from one layer is used as input to the next layer. Such networks are called feedforward neural networks.



\begin{figure}[t]
    \newImageResizeCustom{0.9}{figures/data-analysis/neural-network-architecture.png}
    \caption{Example illustration of a fully-connected neural network. The three dots indicate that neural networks can be defined with an arbitrary number of neurons and hidden layers.}
    \label{fig:neural-net}
\end{figure}


% \begin{figure}[t]
%     \newImageResizeCustom{0.7}{figures/data-analysis/single-neuron-illustration.png}
%     \caption{Illustration of a single artificial neuron, including the inputs, $\mathbf{x}$ and $\mathbf{w}$, the bias, $b$, and the activation function $a(\cdot)$ as well as output weight.}
%     \label{fig:single-neuron}
% \end{figure}


\section{Neural Network Training}
\label{sec:nn-training}
% Read also here: http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent

% The work presented in this thesis uses supervised learning techniques to train a binary classifier that distinguishes between signal-like and background-like events. 
% Therefore, only a single output node is required, that uses a logistic sigmoid as activation function such that $y = \sigma(x)$, where $\sigma(x)$ is defined as in \cref{eq:logistic-sigmoid}.
% The network is trained with MC simulated $pp$ collision events that have ground-truth labels called \emph{target values} in the following. The target value, $t_n$, for a given event $n$ has a value of $t_n = 1$, if the event is a simulated signal event, and $t_n = 0$ otherwise.
% \Minote{}{The above can potentially go into the HWW chapter, as it is specific! Would need to define target values}

The training process is an iterative numerical procedure, during which the free parameters of the network, $\mathbf{W}$, are updated so that the loss function, $L(\mathbf{W})$, is minimized.

To determine the weight updates, gradient-based optimization algorithms are commonly used, that update a given weight based on the gradient of the loss function with respect to that weight, 
\begin{equation}
    \label{eq:gradient-descent}
    \mathbf{W}^{(\tau+1)} = \mathbf{W}^\tau - \eta \grad_{\mathbf{W}^{(\tau)}} L(\mathbf{W}^{(\tau)}),
\end{equation}
where $\mathbf{W}^{(\tau)}$ are the weights at iteration $\tau$.
The size of the update is controlled by the parameter $\eta$, which is known as the \emph{learning rate} and the initial weights, $w^{(0)}$, are chosen randomly.

A common approach is to evaluate the loss for a set of training samples, known as \emph{batches}.\footnote{The gradient descent algorithm that uses batches of data is sometimes referred to as \emph{mini-batch gradient descent}. Other algorithms are frequently used, such as the \emph{stochastic gradient descent} algorithm, that updates the weights after each training sample, or the \emph{gradient descent} algorithm, which evaluates the gradients based on the loss evaluated for the entire training data.} 
The gradients of the resulting loss with respect to each weight are computed with a method known as \emph{backpropagation}~\ccite{Rumelhart_1986}. Backpropagation computes the gradients one layer at a time by applying the chain rule of differentiation, starting from the output layer and iterating backwards.
This is made possible by choosing activation functions and loss functions that are continuously differentiable, so that the NN output is also fully differentiable with respect to each weight.
\TDinote{}{Describe back propagation more?}

% A common approach is to update the weights after a set of training samples, known as \emph{batches}, were passed through the network\footnote{The gradient descent algorithm that uses batches of data is sometimes referred to as \emph{mini-batch gradient descent}. Other algorithms are frequently used, such as the \emph{stochastic gradient descent} algorithm, that updates the weights after each training sample, and \emph{gradient descent} algorithm, which evaluates the gradients of the loss of the entire training data.}. 
% , by first passing the data through the network and computing the loss, and then 
% After each batch of data, the loss is computed and the weights are adjusted according to \cref{eq:gradient-descent}. 
% This requires the computation of the gradient of the loss function, which is a non-trivial task \TD{and cannot be solved analytically}{Think/read about that one}. 
%The computation of the gradients is done with a method called \emph{back propagation}~\ccite{Rumelhart_1986}.
% is therefore used, that derives the gradient of the loss function with respect to each weight, by applying chain-rule differentiation.

The loss function quantifies the difference between the true values of the labelled sample data -- known as \emph{target values}, $t_n$ -- and the values predicted by the current NN model. A typical choice is the sum-of-squares loss function,
\begin{equation}
    \label{eq:mean-squared-loss}
   L^{\text{SOS}}(\mathbf{W}) = \sum _{n=1}^{N}\left(L_n^{\text{SOS}}(\mathbf{W}) \right) = \sum _{n=1}^{N}\left( y(\mathbf{x}_n, \mathbf{W})-t_n \right)^{2},
\end{equation}
where $N$ is the size of the batch, denoted as \emph{batch size}. 

Another commonly used loss function, in particular for binary classification tasks, is the so-called \emph{cross-entropy loss},
\begin{equation}
    \label{eq:cross-entropy-loss}
    L^{\text{CE}}(\mathbf{W}) = \sum _{n=1}^{N}\left(L_n^{\text{CE}}(\mathbf{W}) \right) = \sum _{n=1}^{N}\left( t_n \ln y_n + ( 1 - t_n) \ln (1 - y_n) \right).
\end{equation}

It is also possible to assign a weight, $\omega$, to each training sample, that is applied as a multiplicative factor when computing the loss function and thus controls the relative penalty for incorrect predictions. The loss then becomes
\begin{equation}
    L_{\text{weighted}}(\mathbf{W}) = \sum _{n=1}^{N} \omega_n L_n(\mathbf{W}), 
\end{equation}
where $L_n(\mathbf{W})$ is the non-weighted loss as shown in \cref{eq:cross-entropy-loss,eq:mean-squared-loss}. 
A common choice in HEP is to apply weights so that for each type of physics process the relative contribution in the training data is considered by accounting for the different cross-section normalizations.
The weights may also be used to correct for class imbalances in the training data, for example signal versus background, or to encode some prior knowledge of the importance of different physics processes that goes beyond a flat normalization factor. The latter is useful when for example a particular background process has a low overall normalization but mainly populates the signal sensitive region. In cases like that, providing a larger weight may improve the classification accuracy for that background. 


\section{Hyperparameter optimization} 
In practice, when training a neural network, significant effort needs to be put into finding a suitable set of network parameters and training parameters (collectively known as \emph{hyperparameters}) so that the chosen \emph{performance metric} is optimized and the network converges to a minimum both rapidly and reliably. 
The metric that is optimized actively during the training is the loss function, which is used to assess the convergent behavior of the training. 
The final performance metric that is used to select the trained model is problem dependent and can range from simple estimators, such as the classification accuracy in the case of classification tasks, to more complex assessments, as is typically the case in HEP physics analysis.
In HEP, the performance metric should estimate as close as possible the performance of the final physics measurement. This, however, is complex because the measurement is usually performed with a sophisticated statistical procedure. 
A common approach is therefore to evaluate the performance with simplified estimators extracted from the distribution of the network output, such as the expected discovery significance (as explained in \TDnote{REF}{REF to section}).
To tune the set of hyperparameters, several useful training techniques exist that are explained below.

\paragraph{Learning rate} \mbox{}\\
The learning rate is one of the most crucial hyperparameters as it determines the size of the weight updates. Choosing a learning rate that is too large, can prevent convergence and cause the loss function to fluctuate around the minimum. Too low a learning rate can lead to extremely slow or no convergence. 
One method to mitigate this problem is using a \emph{learning rate schedule}, that adjusts the learning rate based on the progress during the training. As such, a larger learning rate can be used in the beginning of the training, that gets smaller over time.
Typically, the learning rate is chosen by testing different values and observing the behavior of the loss function as a function of the training progress.

% @see: https://ruder.io/optimizing-gradient-descent/
% avoiding getting trapped in their numerous suboptimal local minima
% A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.

\paragraph{Optimizers}\mbox{}\\
There are many variations of the gradient descent algorithm presented in \cref{sec:nn-training}, that can improve the convergence of the training \ccite{ruder2017overview}. The \emph{AdaGrad}~\ccite{adagrad-duchi} algorithm is one such example that is used in the work presented in this thesis. It adapts the learning rate based on the size of the gradients at each parameter, which, in layman's words, leads to rare features in the training data being given more weight than frequently occurring features. This helps to make the training converge faster and more reliably.

\paragraph{Regularization}\mbox{}\\
To avoid overfitting and ensure generalization, several regularization methods exist \TDnote{REFERENCE}{REFERENCE}. 
In the work presented in this thesis, so-called \emph{dropout}~\ccite{srivastava_dropout_2014,DBLP:journals/corr/abs-1207-0580} is used for regularization. In this method, a chosen percentage of neurons along with their connections are randomly disabled (or ``dropped'') for each iteration of the training. This means that during each training iteration a different ``thinned'' version of the original network is effectively used, which reduces the risk of overfitting. 
Other forms of regularization are the so-called $L_2$ regularization, that applies a penalty to the loss function based on the size of the weights, or the more practical approach of early stopping, in which the training is stopped as soon as signs of overfitting are observed. 
Another key factor is the size of the training data itself. The more training samples are available, the less likely it is for the network to be able to adjust to unique features of the training data and thus suffer from overfitting.


\paragraph{Dataset split and cross-validation}\mbox{}\\
In practice, when training a neural network, the available dataset is generally split into multiple subsets, to allow testing the generalizability of the network as well as to avoid model selection bias.
The following three subsets are generally defined, each fulfilling a particular task:
\begin{itemize}
    \item Training set: the subset of data used to fit the model parameters.
    \item Validation set: the subset of data used to tune the set of hyperparameters (for example the learning rate or the number of hidden layers and neurons). As well, it is typically evaluated alongside the training set during the training to ``monitor'' the progress with an unbiased evaluation without affecting the training itself (for example to compare the loss function at each iteration with the loss of the training set). 
    \item Test set: the subset of data that provides the final performance evaluation and final test of generalizability. 
\end{itemize}

A naive approach is to split the dataset once, train the model with the training set, tune the hyperparameters by evaluating the performance on the validation set, and test how well the model performs on the fully independent test set. 
This, however, comes at the cost of loosing statistical precision in each of the sets. 
To overcome this, a method known as \emph{cross validation} can be used, that trains the model in different iterations, each with a different definition of the train, validation, and test set.
Multiple forms of cross validation exist, the one used in the work presented in this thesis is known as \emph{k-fold cross validation}, in which the model is trained in $k$ iterations. A schematic of the dataset split for $k=5$ is shown in \cref{fig:k-fold-method}. 
Cross validation comes with a significant increase in computing time, as $k$ independent models are trained, but it allows exploiting more of the available statistical power in the training and the evaluation of the performance metrics.
% - Two purposes in HEP: maximize training statistics, ability to use full MC set, that is have an unbiased estimator for each MC event.
% - cross validation to avoid any bias from using the prediction of an event that was also learned using the same event.
% - k-fold cross validation to prevent any bias but still exploit significant portion of available data set for training \cref{fig:k-fold-method}.

Another big advantage of cross validation is specific to HEP physics analyses. 
Here, the dataset that is used for the training is usually required for further statistical interpretations. In cases like that, it is best practice to use the test set for further analysis. This removes any potential bias that may occur when data samples are evaluated with a model that was trained with same data sample.
When cross validation is used each data sample falls in one of the test sets, such that the entire dataset can be used for further analysis.
% best practice to avoid using the model output of a data sample if the model was trained with the same data sample. The

\begin{figure}[t]
    \newImageResizeCustom{0.7}{figures/data-analysis/k-fold-illustration.png}
    \caption{Schematic showing the split of the training data into a training (train), validation (val), and test set in a 5-fold cross-validation with interleaved validation and test set.}
    \label{fig:k-fold-method}
\end{figure}
