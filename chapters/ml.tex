%% Resources

% For training and stuff: http://neuralnetworksanddeeplearning.com/chap3.html


% Could summarize this under a bigger chapter:
% DATA ANALYSIS STRATEGIES 

% Intro: Describe dimensionality reduction.

% Machine Learning
% Statistical data analysis (binned likelihood function)
 
% %-------------------------------------------------------------------------------
\chapter{Machine Learning in High Energy Physics}
\label{chap:ml}

- data-driven era, ML also more and more adopted in HEP
- replacing traditional analysis strategies that rely on a series of boolean selections based on single observables.
- Different applications of ML in ATLAS and throughout HEP community in general: hit reconstruction \ccite{PERF-2012-05} or track finding, object identification, classification of entire events. ().
- ML can leverage multiple variables simultaneously.

- Here, used to construct a classifier that predicts the likeliness of an event being a signal. 
- Can be done with supervised learning techniques that use MC simulations as labelled data
- The classifier can be directly used in the final statistical analysis of the distribution

% Description of cut-based analysis
% Traditional data analysis techniques in HEP use a sequence of boolean deci- sions followed by statistical analysis on the selected data. Typically, both the individual decisions and the subsequent statistical analysis are based on the distribution of a single ob- served quantity motivated by physics considerations, which is not easily extended to higher dimensions.
% Within HEP this approach is often referred to as multivariate analysis (MVA); however, outside of physics these techniques would be considered examples of machine learning.


\section{Core Concepts and Terminology in Machine Learning}
% From [1] see above:
- Search for function f : X -> Y, that maps the higher-dimensional observed data X on some target label Y of lower dimension, while optimizing a metric of choosing. 
- The metric is known as loss function and labelled as L(y, f(x)). 

- In a process known as \emph{training}, a learning algorithm is applied to find the function that optimizes the loss function.

- The space of functions to choose from is constrained by a model that can come in various forms such as BDTs, support vector machines, or neural networks (see the following chapter).

- A major goal in machine learning is \emph{generalization}, that is the ability of a trained model to perform well on previously unseen data.  
- If the model does not generalize, that is, if model parameters were chosen based on signatures that are unique to the training data, it is known to be \emph{overfitted}. 
% - This is known as \emph{overtraining} (or \emph{overfitted.})
- To prevent overfitting, various so-called \emph{regularization} techniques can be used during the training. 

%- Their ability to reduce overfitting has been mostly assessed empirically, rather than from theoretical analysis, which is 



\section{Introduction to Neural Networks}
- neural networks are one way to optimise the function in an efficient manner

- illustration of neural network components

- Add formular with weights and biases and so on...

- layers: input, transformations, output
- weights, biases
- activation function, output function(sigmoid)
- taken together is known as the network architecture

- Loss function quantifies the difference between the true values and the current approximated values.


\section{Training Procedure and Hyperparameters}
- In the work presented in this thesis, supervised learning is used to train a classifier, based on the labels (maybe some nice vectors as here: https://cds.cern.ch/record/2791460?ln=en) provided by MC simulations.
%- The classifier is trained by choosing as a target value a boolean, that is true for signal events and false for other type of events. 


- Weight initialisation
- Optimization with back propagation, using gradient descent to update parameters efficiently
    -> Learning rate
    
- Loss function
- Optimisers (stochastic gradient descent is a basic form)
- Regularisation techniques: dropout
- k-fold cross validation to prevent any bias but still exploit significant portion of available data set for training.
    - Create figure for this (with miro!?)
- Early stopping

- The set of parameters that define the network architecture together with the parameters used in the training are together referred to as hyperparameters. 

- A main part of machine learning is to find the most optimal parameters, to optimize the performance. 

- grid search

%\section{Multiclass Classification}
