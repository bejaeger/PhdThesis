%% Resources

% For training and stuff: http://neuralnetworksanddeeplearning.com/chap3.html

% Could summarize this under a bigger chapter:
% DATA ANALYSIS STRATEGIES 

% Intro: Describe dimensionality reduction.

% Machine Learning
% Statistical data analysis (binned likelihood function)

% %---------------------------------------------------------------------
\chapter{Machine Learning in High Energy Physics}
\label{chap:ml}

% for example in the Higgs boson analysis presented in \cref{chap:hww}. 
% , new calibration techniques such as general jet calibration \cite{}, or jet origin identification techniques such as $W$ or $Z$ boson tagging~\cite{ATL-PHYS-PUB-2021-029}. 
% check here for references: https://arxiv.org/pdf/2103.12226.pdf
% - Add examples in one/two sentences here?
% - mention classification task: 
% FROM PHD: "and an event-level classifier using the 4-momenta of a particle collision to determine which process gave rise to the collision."
% - mention supervised learning
% - mention neural networks
% A formal definition of ML was provided in the book ``Machine Learning''~\ccite{mitchell1997machine}:
% ``A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.''
% This definition can be interpreted from the perspective of supervised learning techniques, that are particularly common in HEP physics analyses. 
% Supervised learning relies on labelled sample data which in HEP is provided by MC simulated samples that have ground-truth labels and thus provide the inputs - or ``experience'' - for a computer program to ``learn''.
% The ``tasks T'' in HEP often correspond to classification tasks, the ``performance P'' of which can be measured with a metric that compares the true label with the label predicted by the model. 
% The iterative updating of the model that is being learned by the ``computer program'', so that the metric is optimized, can be thought of as the ``learning'' process, also known as ``training''. 


% Classifying the Standard Model events is a challenging task, as it
% consists of many complicated physics processes. Multi-class machine learning algorithms are well-suited for
% this classification problem. Once an event is classified as likely to be from a known physics process, it can be
% filtered out, and remaining events can be further analyzed for hints of new physics.

The success of Higgs boson measurements depends on the ability to distinguish $pp$ collision events with Higgs boson processes from other types of events, which are typically produced at much higher rates. Given a set of experimentally measured observables, this task can be framed as a classification problem for which modern machine learning (ML) algorithms are particularly well-suited.\footnote{In recent years, ML has become ubiquitous and drives much of human decision-making in many areas of life. This development was facilitated by the growing amount of both computing resources and data, as well as software advancements in the beginning of the 21$^{\text{th}}$ century.}
The main advantage of ML tools is that they can analyze high-dimensional data simultaneously, which in many cases makes them superior to traditional analysis strategies that often rely on a series of boolean selections on a sequence of observables. 
For this reason, more and more traditional HEP analysis techniques are being replaced by modern ML tools, and this trend is expected to continue.\footnote{A complete overview of ML applications in HEP can be found in the literature, for example in \ccite{MLinHEPReview,Guest_2018,MLforNewPhysics}.}
% In recent years, machine learning (ML) has become ubiquitous and drives much of human decision making in many areas of life. 
% The advent of deep neural networks in the beginning of the 21$^{\text{th}}$ century was facilitated by the growing amount of both computing resources and data, as well as software advancements.
% The various ML applications in HEP are detailed in the literature \cite{MLinHEPReview,Guest_2018,MLforNewPhysics}.
% Some examples where ML tools already find adoption in the ATLAS collaboration include hit reconstruction tasks and track finding~\cite{PERF-2012-05}, object identification such as flavor-tagging \cite{ATL-PHYS-PUB-2017-013}, or classification of entire events as performed by various physics analyses of the ATLAS collaboration.
This chapter introduces the concepts necessary to develop a binary classifier by using \emph{supervised learning} techniques with \emph{neural networks}\footnote{Neural networks are the most widely used machine learning models to date, due to their great flexibility and ability to approximate arbitrary, nonlinear functions~\cite{HORNIK1989359}. The latter is known as the universal approximation theorem.}.
This technique is used in the analysis of \HWW decays presented in \cref{chap:hww}, where a \emph{deep neural network} (DNN)\footnote{A deep neural network is conceptually equivalent to a neural network, but is commonly used to denote models with many parameters.} is trained to distinguish VBF, \HWW events from other types of events that are orders of magnitude more likely to occur.

\Cref{sec:core-concepts} introduces the core concepts and relevant terminology used in ML.
\Cref{sec:intro-neural-nets} describes the structure of neural networks, before \cref{sec:nn-training} explains the learning procedure with which the networks are trained. \Cref{sec:hyperpar-opt} provides details on how a neural network model is optimized and deployed in practice.

% This chapter first discusses the core concepts of ML, before introducing neural networks and the details of the learning procedure are described.

%\footnote{The advent of deep neural networks in the beginning of the 21$^{\text{th}}$ century was facilitated by the growing amount of both computing resources and data, as well as software advancements.} 

% This chapter provides an overview of the ML technique of \emph{supervised learning} with \emph{neural networks}, that is relevant for the work presented in this thesis. 

% \Cref{chap:hww} of this thesis describes, how an ML model is optimized to perform binary classification between signal-like and background-like $pp$ collision events. 

% This chapter first provides an overview of the various applications of ML in HEP that exist to date and then discusses the core concepts behind it. Thereafter, the details of ML that are relevant for the work presented in this thesis are discussed, in particular the specifics behind the concept of neural networks, and the details of the learning procedure.

% Description of cut-based analysis
% Traditional data analysis techniques in HEP use a sequence of boolean deci- sions followed by statistical analysis on the selected data. Typically, both the individual decisions and the subsequent statistical analysis are based on the distribution of a single ob- served quantity motivated by physics considerations, which is not easily extended to higher dimensions.
% Within HEP this approach is often referred to as multivariate analysis (MVA); however, outside of physics these techniques would be considered examples of machine learning.

% \section{Applications of Machine Learning in HEP}
% - ML is a fast moving field and more and more applications are being tested and adopted in HEP. The following provides only a very small selection of potential applications in HEP and aims at highlighting the different areas in HEP where ML models can be leveraged. 
% \Minote{}{Put the above in a footnote in the introduction!}
% \TDinote{}{Think whether this is really needed! Leave out for now! Maybe change introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Nice description in the following
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% From ML paper: Section 3.7 in https://arxiv.org/pdf/1807.02876.pdf
% New physics may manifest itself as unusual or rare events. One approach is to accurately identify the Standard
% Model processes and search for anomalies. Classifying the Standard Model events is a challenging task, as it
% consists of many complicated physics processes. Multi-class machine learning algorithms are well-suited for
% this classification problem. Once an event is classified as likely to be from a known physics process, it can be
% filtered out, and remaining events can be further analyzed for hints of new physics.


\section{Core Concepts of Machine Learning}
\label{sec:core-concepts}
The basic task of a machine learning algorithm is to find a function $f: X \rightarrow Y$, that maps some higher-dimensional data $X$ onto some target label $Y$, while optimizing a metric of choosing.
This metric is known as a \emph{loss function} (also called \emph{cost function}).
The function that optimizes the loss function is found in a process referred to as \emph{training}, in which a learning algorithm is applied to a set of sample data, known as \emph{training data}.
In the case of supervised learning, the training data is labelled and the loss function, $L(t_n, f(x))$, quantifies the difference between the true target values of the labelled training data, $t_n$, and the values predicted by the function $f(x)$.
In HEP, the training data consists of MC simulated samples, using their truth information as labels.
% The training data therefore must be labelled which in HEP is easily achieved by using MC simulated samples and their truth information. 
The training proceeds over several cycles through the entire training data, which are known as \emph{epochs}. 
The space of functions to choose from in the training is constrained by a \emph{model} with adjustable parameters that are optimized during the training.
While in the past, many HEP applications were based on \emph{boosted decision trees} (BDTs), the most widely adopted models today are based on (deep) neural networks, which are more powerful discriminators because of their ability to approximate any nonlinear function~\cite{HORNIK1989359}.

% Neural networks, especially deep neural networks (DNNs), are the most widely used machine learning models today and ubiquitous in HEP and many other technology-related fields due to their great flexibility and ability to approximate more complex, nonlinear functions. 
%Details on DNNs are given below, as they are used in the work presented in this thesis.
% % From [1] see above:
% - Search for function f : X -> Y, that maps the higher-dimensional observed data X on some target label Y of lower dimension, while optimizing a metric of choosing. 
% - The metric is known as loss function and labelled as L(y, f(x)). 
% - In a process known as \emph{training}, a learning algorithm is applied to find the function that optimizes the loss function.
% - The space of functions to choose from is constrained by a model that can come in various forms such as BDTs, support vector machines, or neural networks (see the following chapter).

A major goal in machine learning is \emph{generalization}, i.e., the ability of a trained model to perform well on previously unseen data.
The opposite is the case if the model parameters are chosen based on signatures unique to the training data. This is known as \emph{overtraining} (or \emph{overfitting}). To prevent a model from being overfitted, various so-called \emph{regularization} techniques can be used during the training.
Regularization methods are typically tuned during the training procedure explained further below.
%based on how well a trained model generalizes, which can be tested with data not used in the training.


\section{Introduction to Neural Networks}
\label{sec:intro-neural-nets}
Neural networks can be visualized as interconnected layers of artificial neurons\footnote{NNs are therefore also called artificial neural networks (ANNs), which is more descriptive but usually used synonymously with the simple form ``neural networks''.}. \Cref{fig:neural-net} illustrates a fully-connected neural network, where each layer receives inputs only from the previous layer. These types of NNs are known as \emph{feedforward} NNs.
\begin{figure}[ht]
    \newImageResizeCustom{0.9}{figures/data-analysis/neural-network-architecture.pdf}
    \caption[Illustration of a fully-connected neural network.]{Example illustration of a fully-connected neural network. The three dots indicate that neural networks can be defined with an arbitrary number of neurons and hidden layers.}
    \label{fig:neural-net}
\end{figure}
The first layer is called the \emph{input layer} and takes the input vector $\pmb{x}$, the last layer is the \emph{output layer} with output $\pmb{y}$, and the layers in between are known as \emph{hidden layers}. 
Each layer performs a transformation of the data from the respective previous layer based on an \emph{activation function}, labelled as $h^{(l)}(\cdot)$ for the $l^\text{th}$ layer, and weights that connect the artificial neurons to each other. The weights connecting the $(l-1)^{\text{th}}$ layer with the $l^{\text{th}}$ layer can be written as a matrix, $W^{(l)} = \left(w^{(l)}_{jk} \right)$\footnote{In this notation, the weight labelled as $w^{(l)}_{jk}$ connects the $k^\text{th}$ neuron in the $(l-1)^\text{th}$ layer to the $j^\text{th}$ neuron in $(l)^\text{th}$ layer.}. In addition, each neuron except the ones in the input layer has a so-called \emph{bias} weight, described by the bias vector $\pmb{w}^{(l)}_0$.
% . Hence, the activation function of the input layer can be considered as the identity function, $h^{(0)}(\pmb{x}) \equiv \pmb{x}$. 
% The result of the transformation is denoted as $\pmb{z}^{(l)} = h^{(l)}(\cdot)$, which means the output of the neural network corresponds to $ \pmb{z}^{(L)} \equiv \pmb{y}$.
% The artificial neurons are connected via \emph{links} that have weights associated, which are described with the weight matrix $W^{(l)} = \left(w^{(l)}_{jk} \right)$\footnote{In this notation, the weight labelled as $w^{(l)}_{jk}$ connects the $k^\text{th}$ neuron in the $(l-1)^\text{th}$ layer to the $j^\text{th}$ neuron in $(l)^\text{th}$ layer.}, that holds the weights connecting the $(l-1)^{\text{th}}$ layer with the $l^{\text{th}}$ layer.
% In addition, each neuron except the ones in the input layer has a so-called \emph{bias} weight, described with the bias vector $\pmb{w}^{(l)}_0$ for the $l^\text{th}$ layer.
The transformation that the $l^{\text{th}}$ layer performs can then be expressed recursively as,
\begin{equation}
    \label{eq:recursive-neuron-activation}
    \pmb{z}^{(l)} =  h^{(l)} \left( W^{(l)} \pmb{z}^{(l-1)}  + \pmb{w}^{(l)}_{0} \right),
\end{equation}
which implies that the activation function of the input layer can be considered as the identity function, $h^{(0)}(\pmb{x}) \equiv \pmb{x}$, and the output of the neural network is $ \pmb{z}^{(L)} \equiv \pmb{y}$.

The full function that a simple neural network with only one hidden layer and a single output neuron encodes can consequently be written as,
\begin{equation}
    y(\pmb{x}, \pmb{w}) = h^{(2)} \left( W^{(2)} h^{(1)} \left(  W^{(1)} \pmb{x} + \pmb{w}^{(1)}_{0}    \right) + w^{(2)}_{0} \right),
\end{equation}
where the label $\pmb{w}$ in $y(\pmb{x}, \pmb{w})$ is introduced to denote collectively the weights linking the neurons and bias weights.
This function corresponds to a nonlinear function, controlled by a set of adjustable parameters. Here, the weight matrix $W^{(2)}$ is a simple vector and the activation function $h^{(2)}(\cdot)$ is the activation of the output neuron.
% In order to understand this transformation, a single artificial neuron is depicted in \cref{fig:single-neuron}. 
There are different forms of activation functions that can be used.
Feedforward NNs generally use the same function in all hidden layers.
A common choice is the so-called \emph{Rectified Linear Unit} (ReLU) function, defined as
\begin{equation}
    {h(x)= R(x) = \max(0,x)}.    
\end{equation}
A dedicated activation function is typically used for the output node, depending on the problem.
A natural choice for binary classification is a logistic sigmoid,
\begin{equation}
    \label{eq:logistic-sigmoid}
    h(x)= \sigma(x) = {\frac {1}{1+e^{-x}}},
\end{equation}
which restricts the output to values between zero and one and can thus be interpreted probabilistically.
Both functions are shown in \cref{fig:sigmoid-relu}.

\FloatBarrier
\begin{figure}[t]
    \subfloat[Logistic sigmoid] {
        \newImageResizeCustom{0.45}{figures/plots/sigmoid.pdf}
    }
    \subfloat[ReLU] {
        \newImageResizeCustom{0.45}{figures/plots/relu.pdf}
    }
    \caption[Neural network activation functions]{Plots of two typical activation functions used in neural networks.}
    \label{fig:sigmoid-relu}
\end{figure}

% The choice of activation functions as well as the number of layers and neurons is known as the \emph{network architecture} in a feedforward NN. 
%The weights $\pmb{w}$ of the NN are the free parameters that are to be optimized during the training process.

% Quote from: http://neuralnetworksanddeeplearning.com/chap1.html
%- Up to now, we've been discussing neural networks where the output from one layer is used as input to the next layer. Such networks are called feedforward neural networks.
% \begin{figure}[t]
%     \newImageResizeCustom{0.7}{figures/data-analysis/single-neuron-illustration.png}
%     \caption{Illustration of a single artificial neuron, including the inputs, $\pmb{x}$ and $\pmb{w}$, the bias, $b$, and the activation function $a(\cdot)$ as well as output weight.}
%     \label{fig:single-neuron}
% \end{figure}

% \definecolor{mygreen}{HTML}{66AA53}
% \definecolor{myyellow}{HTML}{B8AA1E}
\section{Neural Network Training}
\label{sec:nn-training}
% Read also here: http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent
% The work presented in this thesis uses supervised learning techniques to train a binary classifier that distinguishes between signal-like and background-like events. 
% Therefore, only a single output node is required, that uses a logistic sigmoid as activation function such that $y = \sigma(x)$, where $\sigma(x)$ is defined as in \cref{eq:logistic-sigmoid}.
% The network is trained with MC simulated $pp$ collision events that have ground-truth labels called \emph{target values} in the following. The target value, $t_n$, for a given event $n$ has a value of $t_n = 1$, if the event is a simulated signal event, and $t_n = 0$ otherwise.
% \Minote{}{The above can potentially go into the HWW chapter, as it is specific! Would need to define target values}

The training process is an iterative numerical procedure, during which the free parameters of the network, $\pmb{w}$, are updated so that the loss function, $L(\pmb{w})$, is minimized. 
First, the initial weights, $\pmb{w}^{(0)}$, are chosen randomly. Then, the weights are updated according to gradient-based optimization algorithms that update the weights based on the gradient of the loss function with respect to these weights, 
\begin{equation}
    \label{eq:gradient-descent}
    \pmb{w}^{(\tau+1)} = \pmb{w}^\tau - \eta \nabla_{\pmb{w}^{(\tau)}} L(\pmb{w}^{(\tau)}),
\end{equation}
where $\pmb{w}^{(\tau)}$ denotes the weights at iteration $\tau$ corresponding to one weight update cycle.
The size of the update is controlled by the parameter $\eta$, known as the \emph{learning rate}, which is multiplied with the gradient of the loss function, $\nabla_{\pmb{w}^{(\tau)}} L(\pmb{w}^{(\tau)})$.
The loss function quantifies the difference between the true values (also called \emph{target values}) of the labelled sample data, $t_n$, and the values predicted by the current NN model, $y_n \equiv y(\pmb{x}_n, \pmb{w})$. 
% A typical choice is the sum-of-squares loss function,
% \begin{equation}
%     \label{eq:mean-squared-loss}
%    L^{\text{SOS}}(\pmb{w}) = \sum _{n=1}^{N}L_n^{\text{SOS}}(\pmb{w})= \sum _{n=1}^{N}\left( y(\pmb{x}_n, \pmb{w})-t_n \right)^{2},
% \end{equation}
% where $N$ is the size of the batch, denoted as \emph{batch size}. 
The function commonly used for binary classification tasks is the so-called \emph{cross-entropy loss} defined as
\begin{equation}
    \label{eq:cross-entropy-loss}
    L^{\text{CE}}(\pmb{w}) = - \sum _{n=1}^{N}L_n^{\text{CE}}(\pmb{w}) = \sum _{n=1}^{N}\left( t_n \ln y_n + ( 1 - t_n) \ln (1 - y_n) \right),
\end{equation}
where $n$ runs over the events in one batch that has a size of $N$ (known as \emph{batch size})\footnote{The gradient descent algorithm that uses batches of data is sometimes referred to as \emph{mini-batch gradient descent}. Other algorithms are frequently used, such as the \emph{stochastic gradient descent} algorithm, that updates the weights after each training sample, or the \emph{gradient descent} algorithm, which evaluates the gradients based on the loss evaluated for the entire training data.}, and the target values are set to $t_n = 1$ for simulated signal events and $t_n = 0$ otherwise.
For each event in the training data, a weight, $\omega$, can be applied as a multiplicative factor in the loss function, 
\begin{equation}
    L_{\text{weighted}}^{\text{CE}}(\pmb{w}) = \sum _{n=1}^{N} \omega_n L_n^{\text{CE}}(\pmb{w}).
\end{equation}
%where $L_n(\pmb{w})$ is the non-weighted loss as shown in \cref{eq:cross-entropy-loss}.
This allows controlling the relative penalty for incorrect predictions at the level of individual training samples, which is particularly useful for HEP, where different processes in the training data have different effects on the final analysis measurement making it useful to treat them differently.
% A common choice in HEP is to apply weights so that for each type of physics process the relative contribution in the training data corresponds to their cross-section normalizations.
A common choice in HEP is to apply weights so that the contribution of each physics process is normalized to its theoretical cross-section prediction.
The weights may also be used to correct for class imbalances in the training data, for example signal versus background, or to encode some prior knowledge of the importance of different physics processes that goes beyond a flat normalization factor. The latter is useful for example if a particular background process has a low overall normalization but mainly populates the signal sensitive region, or if the expected contribution of a background process involves large uncertainties.
In such cases, assigning a larger weight for a particular background process may improve the classification accuracy and thus improve the measurement.

The gradients of the loss with respect to each weight are computed with a method known as \emph{backpropagation}~\cite{Rumelhart_1986}. Backpropagation computes the gradients one layer at a time by applying the chain rule of differentiation, starting from the output layer and iterating backwards.
It is instructive to explain the algorithm using the example of a simple NN with three layers and only one node each. Such a network is visualized in \cref{fig:simple-nn}, including example calculations of the gradients of the loss with respect to the first and second weight.
As can be seen, the chain rule is applied once (twice) for calculating the gradient $\frac{\partial L}{\partial w_2}$ $\left( \frac{\partial L}{\partial w_1}\right)$. 
This illustrates the key feature of backpropagation, which is that gradients calculated in a previous step can be reused, making the algorithm computationally efficient.
This simple case can be easily extended to more complex scenarios with more layers and nodes.
It is important to note that the backpropagation algorithm relies on the activation functions and loss functions being continuously differentiable, so that the NN output is fully differentiable with respect to each weight. This limits the space of functions to choose from.
% A common approach is to update the weights after a set of training samples, known as \emph{batches}, were passed through the network\footnote{The gradient descent algorithm that uses batches of data is sometimes referred to as \emph{mini-batch gradient descent}. Other algorithms are frequently used, such as the \emph{stochastic gradient descent} algorithm, that updates the weights after each training sample, and \emph{gradient descent} algorithm, which evaluates the gradients of the loss of the entire training data.}. 
% , by first passing the data through the network and computing the loss, and then 
% After each batch of data, the loss is computed and the weights are adjusted according to \cref{eq:gradient-descent}. 
% This requires the computation of the gradient of the loss function, which is a non-trivial task \TD{and cannot be solved analytically}{Think/read about that one}. 
%The computation of the gradients is done with a method called \emph{back propagation}~\ccite{Rumelhart_1986}.
% is therefore used, that derives the gradient of the loss function with respect to each weight, by applying chain-rule differentiation.

\begin{figure}[ht]
    \subfloat[Illustration of a simple neural network.] {
        \newImageResizeCustom{0.7}{figures/data-analysis/simple-nn.pdf}
    }
    \vspace{2em}
    \begin{align*}
        %\stackrel{\text{(1)}}{=}
        \frac{\partial L}{{\color{cyan}\partial  w_2}} &=  {\color{red}\frac{\partial L}{\partial y}} \frac{\partial y}{{\color{cyan}\partial w_2}} \\
        \frac{\partial L}{{\color{blue} \partial w_1}} &=   {\color{red}\frac{\partial L}{\partial y}} \frac{\partial y}{{\color{blue}\partial w_1}} = {\color{red}\frac{\partial L}{\partial y}} \frac{\partial y}{\partial z_1} \frac{\partial z_1}{{\color{blue}\partial w_1}}
    \end{align*}
    \subfloat[Example gradient calculations]{
        \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad
        }
    \caption[Illustration of a simple neural network.]{Example of a simple neural network used to illustrate the backpropagation algorithm. More information can be found in the text.}
    \label{fig:simple-nn}
\end{figure}

\section{Hyperparameter Optimization and Training Procedure} 
\label{sec:hyperpar-opt}
In practice, when developing a neural network, significant effort needs to be put into finding a suitable set of network parameters and training parameters (collectively known as \emph{hyperparameters}) so that the chosen \emph{performance metric} is optimized and the network converges to a minimum both rapidly and reliably. 
While some parameters are crucial for a successful training and suboptimal settings can be detected quite easily, such as the case for the learning rate, other parameters are more difficult to optimize and their optimization typically requires an iterative workflow in which different values are tested. 
Several useful training techniques and learning algorithms exist, that make finding a suitable training configuration easier.

\subsubsection{Performance metrics}
% The metric that is optimized direclty during the training is the loss function, which is used to assess the convergent behavior of the training. 
% The final performance metric, that is used to select the model, is problem dependent and can range from simple estimators, such as the classification accuracy in the case of classification tasks, to more complex assessments, as is typically the case in HEP physics analysis.
% In HEP, the performance metric should estimate as close as possible the performance of the final physics measurement. This, however, is complex because the measurement is usually performed with a sophisticated statistical procedure. 
% A common approach is therefore to evaluate the performance with simplified estimators extracted from the distribution of the network output, such as the expected discovery significance (as explained in \cref{sec:hypothesis-testing}).
% Several useful training techniques and learning algorithms exist, that make finding a suitable training configuration easier.

%%%%%%%%%%%%%%%%%%%%%%%%%% Take the following in consideration and write a separate "optimization/performance metrics" subsection (copied from hww section)
The performance metric used to select the model is problem dependent.
In typical ML applications, the performance of a binary classifier is often estimated with measures such as the \emph{accuracy}, the percentage of correct positive and negative assignments, or the area under the receiver operating characteristic (ROC) curve~\cite{BRADLEY19971145}. 
For HEP applications, other metrics are more meaningful. In the case of a physics search, the discovery significance of the signal process over the background process directly determines the sensitivity and thus performance of an analysis. 
For a physics measurement, the uncertainty on the final measurement quantity may be used to assess the performance of a neural network model.
% \footnote{Ideally in HEP, the full statistical analysis that is performed to produce the final measurement results should be considered as a final performance metric of a model. This, however, is typically extremely computing intensive and often there are simple alternatives that approximate the final results very well.}

It would be natural to directly apply the aforementioned performance metrics as a loss function and optimize based on them when training the neural network.
However, this is not possible for most metrics because the loss function must be differentiable with respect to the parameters of the network in order to perform backpropagation. The discovery significance, for example, is a discrete function of the number of signal and background events and not differentiable in its simple form.\footnote{New studies explore the possibility to redefine the discovery significance to make it differentiable and directly use it in the training~\cite{ELWOODZ0INML}.}
% They are instead used as a performance metric to select the best performing model and have no impact on the training itself.

To validate the methodology, it is therefore meaningful to show that the loss function optimized during the training is correlated with the performance metric. 
\Cref{fig:loss-vs-sign} shows the anti-correlating behavior between the cross-entropy loss, as defined in \cref{eq:cross-entropy-loss}, and the discovery significance, as defined in \cref{eq:discovery-significance}.

\FloatBarrier
\begin{figure}[t]
    \newImageResizeCustom{0.6}{figures/data-analysis/loss_v_significance_thesis.pdf}
    \caption[Loss and significance as a function of training epochs.]{Cross-entropy loss and discovery significance as a function of training epochs, averaged over three separate trainings. The shaded band corresponds to the standard deviation of the three trainings. The discovery significance is calculated based on a histogram with 20 bins of the DNN output distribution by taking the square root of the sum of the significance calculated in each of the bins according to \cref{eq:discovery-significance}.}
    \label{fig:loss-vs-sign}
\end{figure}
%For binary classification, the cross-entropy loss is most commonly chosen. 
% The metric used during the training must be differentiable with respect to the parameters of the model in order to perform backpropagation. 
% The training itself is based on yet another metric that must be differentiable with respect to the parameters of the model in order to perform backpropagation. 
% The parameters of the model are learned by minimizing the cross-entropy loss. 
% The loss function must be fully differentiable with respect to the parameters of the model
% Add differentiable loss function here?

\subsubsection{Learning rate}
The learning rate is one of the most crucial hyperparameters that must be tuned for the training algorithm to succeed. Choosing a learning rate that is too large can prevent convergence and cause the loss function to fluctuate around the minimum. Too low a learning rate can lead to extremely slow or no convergence. 
One method to mitigate this problem is using a \emph{learning rate schedule} that adjusts the learning rate based on some evaluation metrics computed during the training. A larger learning rate that gets smaller over time can hence be used in the beginning of the training. 
Other more sophisticated approaches use so-called \emph{optimizers} during the training that typically adjust the learning rate at the level of each parameter.
% Typically, the final learning rate is chosen by testing different values and observing the behavior of the loss function as a function of the training progress.

% @see: https://ruder.io/optimizing-gradient-descent/

\subsubsection{Optimizers}
There are many variations of optimizers that can improve the convergence of the gradient descent algorithm. An overview of different methods is given for example in \ccite{ruder2017overview}. The \emph{AdaGrad}~\cite{adagrad-duchi} algorithm is one such example and used in the work presented in this thesis. It adapts the learning rate individually for each parameter by introducing a matrix, $\pmb{G}_{\tau} = \sum_{t=0}^{\tau} \left( \nabla_{\pmb{w}^{(t)}} L(\pmb{w}^{(t)}) \right)^2$, that holds the sum of squares of the past gradients for each parameter. 
The weight updates are then given as,
\begin{equation}
    \pmb{w}^{(\tau+1)} = \pmb{w}^\tau - \frac{\eta}{\sqrt{\pmb{G}_\tau + \epsilon}}  \nabla_{\pmb{w}^{(\tau)}} L(\pmb{w}^{(\tau)}), 
\end{equation}
where $\epsilon$ is a small, non-zero floating-point number used to avoid numerical instability.
Introducing this parameter-specific learning rate largely reduces the dependence on the choice of the initial learning rate, as rare features in the training data are automatically given more importance than frequently occurring ones, which helps to make the training converge faster and more reliable.

% The intuition behind the algorithm: From https://towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d
% AdaGrad decays the learning rate very aggressively (as the denominator grows). As a result, after a while, the frequent parameters will start receiving very small updates because of the decayed learning rate.
% Decay the learning rate for parameters in proportion to their update history (more updates means more decay).

\subsubsection{Regularization}
There are several regularization methods to avoid overfitting and ensure generalization. 
In the work presented in this thesis, so-called \emph{dropout}~\cite{srivastava_dropout_2014,DBLP:journals/corr/abs-1207-0580} is used for regularization. In this method, a chosen percentage of neurons along with their connections are randomly disabled (or ``dropped'') for each iteration of the training. This means that during each training iteration a different ``thinned'' version of the original network is used effectively, which reduces the risk of overfitting. 
Other forms of regularization are the so-called $L_2$ regularization, that applies a penalty to the loss function based on the overall size of the weights, or the more practical approach of early stopping, in which the training is stopped as soon as signs of overfitting are observed. 
Another key factor is the size of the training data itself. The more training samples are available, the less likely it is for the network to be able to adjust to unique features of a particular training dataset and thus suffer from overfitting.


\subsubsection{Dataset split and cross-validation}
\label{subsec:par:kfold-crossval}
The available dataset for training is generally split into multiple subsets to allow testing the generalizability of the network as well as to avoid model selection bias.
The following three subsets are typically defined, each fulfilling a particular task:
\begin{itemize}
    \item Training set: the subset used to fit the model parameters.
    \item Validation set: the subset used to tune the set of hyperparameters and test the generalization of the model. As well, it is evaluated alongside the training set during the training to ``monitor'' the progress with an unbiased evaluation without affecting the training itself. This is useful for identifying signs of overtraining.
    \item Test set: the subset used to deploy the model. 
\end{itemize}

A naive approach is to split the dataset once, train the model with the training set, tune the hyperparameters by evaluating the performance on the validation set, and test how well the model performs on the fully independent test set. 
This, however, comes at the cost of loosing statistical precision in each of the sets. 
To overcome this, a method known as \emph{cross validation} can be used that trains multiple models that are later used in combination. Each model has a different definition of the training, validation, and test set.
There are multiple forms of cross validation. The one used in the work presented in this thesis is referred to as \emph{mixed k-fold cross validation}, in which $k$ models are trained. A schematic of the dataset split for $k=5$ is shown in \cref{fig:k-fold-method}. 
\begin{figure}[ht]
    \newImageResizeCustom{0.7}{figures/data-analysis/k-fold-illustration.pdf}
    \caption[Schematic of the different sets in a 5-fold cross-validation procedure.]{Schematic showing the split of the training data into a training (train), validation (val), and test set in a 5-fold cross-validation with interleaved validation and test sets.}
    \label{fig:k-fold-method}
\end{figure}
While cross validation comes with a significant increase in computing time, it allows exploiting more of the available statistical power in the training and the evaluation of the performance metrics.
% - Two purposes in HEP: maximize training statistics, ability to use full MC set, that is have an unbiased estimator for each MC event.
% - cross validation to avoid any bias from using the prediction of an event that was also learned using the same event.
% - k-fold cross validation to prevent any bias but still exploit significant portion of available dataset for training \cref{fig:k-fold-method}.
Another major advantage of cross validation is particularly important for HEP physics analyses. 
Here, the dataset used for the training is usually required for further statistical interpretations. In such cases, the test set must be used for further analysis to remove potential biases that may occur if the training set or the validation set were used.\footnote{If the training set was used in the inference step, a bias is potentially introduced because the samples are evaluated with a model that was trained with the same sample. If the validation set is used, the model was specifically chosen based on a hyperparameter optimization procedure, which also introduces a potential bias.}
When cross validation is used each training sample falls in one of the test sets, such that the entire training data can be used for further analysis while avoiding potential biases. 
In the analysis of \HWW decays presented in the next chapter, each event in the actual data is evaluated with a randomly assigned fold so that each of the folds is equally represented in the final network output distribution. 
% best practice to avoid using the model output of a data sample if the model was trained with the same data sample. The
