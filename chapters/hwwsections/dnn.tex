%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copied from previous \section{Neural Network Training}
The work presented in this thesis uses supervised learning techniques to train a binary classifier that distinguishes between signal-like and background-like events. 
For such a purpose, only a single output node is required, that uses a logistic sigmoid as activation function such that $y = \sigma(x)$, where $\sigma(x)$ is defined as in \cref{eq:logistic-sigmoid}.
The network is trained with simulated $pp$ collision events that have ground-truth labels called \emph{target values} in the following. The target value, $t_n$, for a given event $n$ has a value of $t_n = 1$, if the simulated event is a signal event, and $t_n = 0$ otherwise.

% From PAPER
% DNN that is implemented through Keras [116] and 341 TensorFlow [117],

\subsection{Optimization and performance metrics}
The task at hand is to construct a function that separates the VBF signal process as well as possible from the background processes.
To this end, the cross entropy loss is used as a measure of the separation and the parameters of the neural network model are learned using backpropagation.
In this analysis, the more meaningful measure of the separation is the expected discovery significance (see \cref{chap:statistics}) of the VBF signal process over the background processes. 
It would therefore be natural to directly optimise the discovery significance when training the neural network. 
However, this is not trivial because the loss function must be differentiable with respect to the parameters of the network\footnote{There are attempts to achieve exactly this, as shown in \ccite{ELWOODZ0INML}.} which is not the case for the discovery significance as it is a discrete function of the number of signal and background events. 
In practice, the cross entropy loss is highly anti-correlated with the discovery significance, which is shown in \cref{FIG}.
The training is therefore performed based on the cross entropy loss and the discovery significance is used only to evaluate the trained model. The significance is computed based on the distribution of the output of the neural network. 
% \begin{itemize}
%     \item The cross entropy loss is used during the training to learn the parameters of the neural network. 
%     \item The discovery significance shown in \cref{eq:discovery-significance} is used as an evaluation metric of a trained model.
%     \item Information about systematic uncertainties is included in the discovery significance to perform the final optimisation and choose the neural network. 
% \end{itemize}

% The training of the neural network itself, however, uses the cross entropy loss function to learn the optimal parameters. This function must be differentiable with respect to the parameters of the model in order to use backpropagation. 
% Therefore, it is not trivial to use the discovery significance in the training of the neural network itself.
% The parameters of the neural network are learned by using backpropagation to minimize the cross entropy loss. 

% The task at hand is to construct a function that separates signal processes as well as possible from background processes, based on the statistical significance of the signal process over the background processes.

% During the training of the neural network, a differentiable loss function is required in order to perform backpropagation.
% The loss function must be fully differentiable with respect to the parameters of the model in order to perform gradient descent optimisation.
% This requires the definition of a metric that can be optimised to achieve exactly this. 
% For binary classification, the cross entropy loss is the most common choice and also used in this analysis during the training.
% FROM ARTICLE
% parameters learned by the model are determined by minimizing a chosen loss function.
% - optimization during training: needs to be differentiable w.r.t. network parameters  -> loss function
% - ultimate figure of merit: likelihood fit
% - technically unfeasible
% - simple significance can be used
% - simple significance with added uncertainties!
% -> show that it is highly correlated with the loss function of the network
% - Plot with loss vs significance

\subsection{Training optimization}

multidimensional optimization problem:
ML related
- network architecture
- learning rate (batch size)
- optimizer
- regularization technique
- choice of k-fold method
Physics related
- set of input features (observables)
- weights corresponding to different processes in training data set



- Historical developments, features freezes, limitations of computing resources/turnaround time

- Flow chart for historical strategy:

Fixed: batchsize, reLU activation, sigmoid as output activation, cross-entropy loss

STAGE 1 OPTIMIZATION (FOM: simple significance, 80/20 split)
Temporary: regularisation, adagrad (adaptive learning rate! (no need to overoptimize it)), architecture
-> Choose input variables. 

Fixed: batchsize, input features

STAGE 2 OPTIMIZATION (FOM: simple significance, 80/20 split)
-> Test optimizer -> Adagrad is good
-> Test different architectures -> A larger one was better
-> Double-check regularisation -> adjust dropout

Fixed: batchsize, input features, optimizer, architecture, regularisation

STAGE 3 OPTIMIZATION (FOM: simple significance including systematics!, k-fold)
-> Choice of k-fold: try to go higher than 5-fold. No favorable results and more bookkeeping, keep 5-fold
-> Physics weights optimization (always scanning learning rate) -> choose weights!

Fixed: Everything


Model choice: Choose model with highest significance in validation set!
Model validation: Validate model comparing against test set!


% \subsubsection{Neural network hyperparameter optimization}
% %\subsection{Prospect Studies for Common VBF and ggF HWW 2-jet Analysis with Multiclass Classification}
% ML related
% - network architecture
% - learning rate (batch size)
% - optimizer
% - regularization technique
% - choice of k-fold method
% \subsubsection{Physics input optimization}
% Physics related
% - set of input features (observables)
% - weights corresponding to different processes in training data set


\subsection{Final model validation}
- compare distribution of different subsets to double-check for overtraining
