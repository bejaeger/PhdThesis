%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copied from previous \section{Neural Network Training}
The work presented in this thesis uses supervised learning techniques to train a binary classifier that distinguishes between signal-like and background-like events. 
For such a purpose, only a single output node is required, that uses a logistic sigmoid as activation function such that $y = \sigma(x)$, where $\sigma(x)$ is defined as in \cref{eq:logistic-sigmoid}.
The network is trained with simulated $pp$ collision events that have ground-truth labels called \emph{target values} in the following. The target value, $t_n$, for a given event $n$ has a value of $t_n = 1$, if the simulated event is a signal event, and $t_n = 0$ otherwise.

% From PAPER
% DNN that is implemented through Keras [116] and 341 TensorFlow [117],

\subsection{Optimization and performance metrics}
The task at hand is to construct a function that separates signal processes as well as possible from background processes.

% Add differentiable loss function here?

In typical ML applications, the performance of such a binary classifier is often estimated with measures such as the \emph{accuracy}, the percentage of correct positive and negative assignments, or the area under the receiver operating characteristic (ROC) curve~\cite{BRADLEY19971145}. 
For HEP applications, other metrics are more suitable. In the case of a physics search, the discovery significance of the signal process over the background process determines the sensitivity and thus performance of the analysis. 

- For physics measurements, the uncertainty on the final measurement quantity is what becomes most relevant. 

- Ideally, the full statistical analysis that is performed to produce the final measurement results should be considered as a performance metric of a neural network model.
- This, however, is extremely computing intensive and there are good alternatives that approximate the final results well.

- Loss function vs significance: could add significance as loss\cite{ELWOODZ0INML}

- In this analysis:

\begin{itemize}
    \item Use cross entropy during the training to update the parameters. 
    \item Use statistical significance calculated based on the distribution of the output of the model. 
    \item Add information about systematic uncertainties to significance -> Choose model based on that
\end{itemize}

- Use in final analysis


% The task at hand is to construct a function that separates signal processes as well as possible from background processes, based on the statistical significance of the signal process over the background processes.

During the training of the neural network, a differentiable loss function is required in order to perform backpropagation.
The loss function must be fully differentiable with respect to the parameters of the model in order to perform gradient descent optimisation.
This requires the definition of a metric that can be optimised to achieve exactly this. 
For binary classification, the cross entropy loss is the most common choice and also used in this analysis during the training.


% FROM ARTICLE
% parameters learned by the model are determined by minimizing a chosen loss function.

- optimization during training: needs to be differentiable w.r.t. network parameters  -> loss function

- ultimate figure of merit: likelihood fit
- technically unfeasible

- simple significance can be used
- simple significance with added uncertainties!

-> show that it is highly correlated with the loss function of the network
- Plot with loss vs significance

\subsection{Training optimization}

multidimensional optimization problem:
ML related
- network architecture
- learning rate (batch size)
- optimizer
- regularization technique
- choice of k-fold method
Physics related
- set of input features (observables)
- weights corresponding to different processes in training data set



- Historical developments, features freezes, limitations of computing resources/turnaround time

- Flow chart for historical strategy:

Fixed: batchsize, reLU activation, sigmoid as output activation, cross-entropy loss

STAGE 1 OPTIMIZATION (FOM: simple significance, 80/20 split)
Temporary: regularisation, adagrad (adaptive learning rate! (no need to overoptimize it)), architecture
-> Choose input variables. 

Fixed: batchsize, input features

STAGE 2 OPTIMIZATION (FOM: simple significance, 80/20 split)
-> Test optimizer -> Adagrad is good
-> Test different architectures -> A larger one was better
-> Double-check regularisation -> adjust dropout

Fixed: batchsize, input features, optimizer, architecture, regularisation

STAGE 3 OPTIMIZATION (FOM: simple significance including systematics!, k-fold)
-> Choice of k-fold: try to go higher than 5-fold. No favorable results and more bookkeeping, keep 5-fold
-> Physics weights optimization (always scanning learning rate) -> choose weights!

Fixed: Everything


Model choice: Choose model with highest significance in validation set!
Model validation: Validate model comparing against test set!


% \subsubsection{Neural network hyperparameter optimization}
% %\subsection{Prospect Studies for Common VBF and ggF HWW 2-jet Analysis with Multiclass Classification}
% ML related
% - network architecture
% - learning rate (batch size)
% - optimizer
% - regularization technique
% - choice of k-fold method
% \subsubsection{Physics input optimization}
% Physics related
% - set of input features (observables)
% - weights corresponding to different processes in training data set


\subsection{Final model validation}
- compare distribution of different subsets to double-check for overtraining
